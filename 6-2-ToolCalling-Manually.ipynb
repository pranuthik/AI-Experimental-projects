{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build Interactive LLM Agents with Tools**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **15** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll explore the powerful capabilities of tool calling in large language models (LLMs) to build advanced AI agents that can dynamically interact with users. Using the LangChain framework, you’ll learn how to build an interactive agent that responds to user queries by selecting and executing the right function at the right time. This hands-on approach will help you understand how LLMs can be extended with real-world functionality, bridging natural language understanding with dynamic, tool-based actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "- [Objectives](#Objectives)\n",
    "- [Setup](#Setup)\n",
    "    - [Installing Required Libraries](#Installing-Required-Libraries)\n",
    "    - [Importing Required Libraries](#Importing-Required-Libraries)\n",
    "- [Creating Custom Tools with LangChain](#Creating-Custom-Tools-with-LangChain)\n",
    "    - [Anatomy of a tool](#Anatomy-of-a-tool)\n",
    "    - [Key components](#Key-components)\n",
    "    - [Defining an add function](#Defining-an-add-function)\n",
    "    - [Add tools to the LLM](#Add-tools-to-the-LLM)\n",
    "    - [Create more Tools](#Create-more-tools)\n",
    "    - [Testing the functions](#Testing-the-functions)\n",
    "    - [Add new tools to LLM](#Add-new-tools-to-LLM)\n",
    "- [Interacting with the Model](#Interacting-with-the-Model)\n",
    "    - [Craft the user query](#Craft-the-user-query)\n",
    "    - [Invoke the model](#Invoke-the-model)\n",
    "    - [Parse tool calls](#Parse-tool-calls)\n",
    "    - [Invoke the tool](#Invoke-the-Tool)\n",
    "    - [Generate a final answer from chat history](#Generate-a-final-answer-from-chat-history)\n",
    "- [Building an Agent](#Building-an-Agent)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Exercises](#Exercises)\n",
    "    - [Exercise 1: Create a New Tool](#Exercise-1:-Create-a-new-tool)\n",
    "    - [Exercise 2: Tool Calling with an LLM](#Exercise-2:-Tool-calling-with-an-LLM)\n",
    "    - [Exercise 3: Create a tip calculating agent](#Exercise-3:-Create-a-tip-calculating-agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Initialize a chat model for tool interactions\n",
    " - Define and bind custom tools to the LLM for expanded functionality\n",
    " - Use mapping dictionaries for dynamic function calls\n",
    " - Extract tool names and functions for precise function calls\n",
    " - Build agent classes that manage the entire tool-calling process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`langchain`](https://python.langchain.com/docs/introduction/) is the framework you will build the agent on.\n",
    "*   [`langchain-openai`](https://pypi.org/project/langchain-openai/) is a partner package of LangChain and integrates OpenAI LLMs to the framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain===0.3.25) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai===0.3.19) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain-ibm\n",
      "  Downloading langchain_ibm-0.3.15-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting ibm-watsonx-ai<2.0.0,>=1.3.28 (from langchain-ibm)\n",
      "  Downloading ibm_watsonx_ai-1.3.31-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.39 in /opt/conda/lib/python3.12/site-packages (from langchain-ibm) (0.3.69)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.32.3)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /opt/conda/lib/python3.12/site-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (0.28.1)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.12/site-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.3.0)\n",
      "Collecting pandas<2.3.0,>=0.24.2 (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2024.12.14)\n",
      "Collecting lomond (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading lomond-0.3.3-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting tabulate (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (24.2)\n",
      "Collecting ibm-cos-sdk<2.15.0,>=2.12.0 (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading ibm_cos_sdk-2.14.2.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cachetools (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (4.12.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (2.10.6)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (0.14.0)\n",
      "Collecting ibm-cos-sdk-core==2.14.2 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading ibm_cos_sdk_core-2.14.2.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.14.2 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading ibm_cos_sdk_s3transfer-2.14.2.tar.gz (139 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jmespath<=1.0.1,>=0.10.0 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /opt/conda/lib/python3.12/site-packages (from ibm-cos-sdk-core==2.14.2->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.9.0.post0)\n",
      "Collecting requests (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.23.0)\n",
      "Collecting numpy>=1.26.0 (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading numpy-2.3.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (3.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.12/site-packages (from lomond->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (1.3.1)\n",
      "Downloading langchain_ibm-0.3.15-py3-none-any.whl (32 kB)\n",
      "Downloading ibm_watsonx_ai-1.3.31-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m165.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading cachetools-6.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading numpy-2.3.1-cp312-cp312-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m194.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
      "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.14.2-py3-none-any.whl size=77291 sha256=16e3cf1e21dd4116b0b38cebf9210add65f82372453ec105f2cdd095ec527103\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/47/ce/0a/7a834776049dbe8b53cba6b470fa717724cfb8ef5f0e359ce3\n",
      "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?done\n",
      "\u001b[?25h  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.14.2-py3-none-any.whl size=662162 sha256=0a2d705ee7bbae025c66c9a95935e12fe1662a94c6dc4798dacce8c3b07dd620\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/05/f9/9c/e668aad6eeda037e6cb74bffe26af8483254843ed22ac4af74\n",
      "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.14.2-py3-none-any.whl size=90260 sha256=ea6b079b5d52d5f3490f008361b451783a318711f5725853b0dd837b673bc84a\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/23/d9/a9/6a60e68d55baa171782894a40f278aa4c93ba6548aefffc151\n",
      "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
      "Installing collected packages: tzdata, tabulate, requests, numpy, lomond, jmespath, cachetools, pandas, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk, ibm-watsonx-ai, langchain-ibm\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "Successfully installed cachetools-6.1.0 ibm-cos-sdk-2.14.2 ibm-cos-sdk-core-2.14.2 ibm-cos-sdk-s3transfer-2.14.2 ibm-watsonx-ai-1.3.31 jmespath-1.0.1 langchain-ibm-0.3.15 lomond-0.3.3 numpy-2.3.1 pandas-2.2.3 requests-2.32.4 tabulate-0.9.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain===0.3.25 | tail -n 1\n",
    "%pip install langchain-openai===0.3.19 | tail -n 1\n",
    "\n",
    "%pip install langchain-ibm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "Recommendation:Import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the language model that will power your tool calling capabilities. This code sets up a GPT-4o-mini model using the OpenAI provider through LangChain's interface, which you'll use to process queries and decide which tools to call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "\n",
    "This lab uses LLMs provided by Watsonx.ai and OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges. \n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses `ChatOpenAI` and `ChatWatsonx` modules from `langchain`. Both configurations are shown below with instructions. **Replace all instances** of both modules with the completed modules below throughout the lab. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import ChatWatsonx\n",
    "openai_llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key = \"your openai api key here\",\n",
    ")\n",
    "watsonx_llm = ChatWatsonx(\n",
    "    model_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Tools with LangChain\n",
    "\n",
    "### Anatomy of a tool\n",
    "\n",
    "Let's provide the basic building blocks of a tool, consider the following tool:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def tool_name(input_param: input_type) -> output_type:\n",
    "   \"\"\"\n",
    "   Clear description of what the tool does.\n",
    "   \n",
    "   Args:\n",
    "       input_param (input_type): Description of this parameter\n",
    "   \n",
    "   Returns:\n",
    "       output_type: Description of what is returned\n",
    "   \"\"\"\n",
    "   # Function implementation\n",
    "   result = process(input_param)\n",
    "   return result\n",
    "```\n",
    "\n",
    "### Key components\n",
    "\n",
    "You'll use the following key components\n",
    "\n",
    "**@tool decorator**\n",
    "   - Registers the function with LangChain\n",
    "   - Creates tool attributes (.name, .description, .func)\n",
    "   - Generates JSON schema for validation\n",
    "   - Transforms regular functions into callable tools\n",
    "\n",
    "**Function name**\n",
    "   - Used by LLM to select appropriate tool\n",
    "   - Used as reference in chains and tool mappings\n",
    "   - Appears in tool call logs for debugging\n",
    "   - Should clearly indicate the tool's purpose\n",
    "\n",
    "**Type annotations**\n",
    "   - Enable automatic input validation\n",
    "   - Create schema for parameters\n",
    "   - Allow proper serialization of inputs/outputs\n",
    "   - Help LLM understand required input formats\n",
    "\n",
    "**Docstring**\n",
    "   - Provides context for the LLM to decide when to use the tool\n",
    "   - Documents parameter requirements\n",
    "   - Explains expected outputs and behavior\n",
    "   - Is critical for tool selection by the LLM\n",
    "\n",
    "6. **Implementation**\n",
    "   - Executes the actual operation\n",
    "   - Handles errors appropriately\n",
    "   - Returns properly formatted results\n",
    "   - Should be efficient and robust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an add function\n",
    "\n",
    "Now use this tool framework to create a custom tool that enables the LLM to perform basic addition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add a and b.\n",
    "    \n",
    "    Args:\n",
    "        a (int): first integer to be added\n",
    "        b (int): second integer to be added\n",
    "\n",
    "    Return:\n",
    "        int: sum of a and b\n",
    "    \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator wraps the `add()` function in LangChain's predefined tool schema. See more about defining custom LangChain tools [here](https://python.langchain.com/docs/how_to/custom_tools/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tools to the LLM\n",
    "\n",
    "Let's connect and bind the function to the chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `bind_tools(tools)` method to connect a list of tools to the LLM for use. From now on, whenever the call is invoked, the model (with tools) will recognize and use the add tool whenever it needs to compute a sum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create more tools\n",
    "\n",
    "Let's create some more basic arithmetic tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def subtract(a: int, b:int) -> int:\n",
    "    \"\"\"Subtract b from a.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b:int) -> int:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "\n",
    "Let's setup a way to test your tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_map = {\n",
    "    \"add\": add, \n",
    "    \"subtract\": subtract,\n",
    "    \"multiply\": multiply\n",
    "}\n",
    "\n",
    "input_ = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 2\n",
    "}\n",
    "\n",
    "tool_map[\"add\"].invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LangChain's built in `.invoke(inputs)` method, you can test each tool built with dynamic inputs.Test each tool with the preceding code block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new tools to LLM\n",
    "\n",
    "Let's add all three tools to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can the same method to bind tools to the LLM, enabling more arithmetic capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Craft the user query\n",
    "\n",
    "Now that you've setup an LLM with basic tool integrations, it's time to introduce user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 3 + 2?\"\n",
    "chat_history = [HumanMessage(content=query)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,setup the question (user query). Then,initialize a `chat_history` array that will contain the entire conversation between user and LLM. In this chat history, you insert the `query` in a `HumanMessage` wrapper that tells LangChain and the model: \"This message came from the user.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the model\n",
    "\n",
    "Now let's run the model with the context (chat history) that contains the user query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "response_1 = llm_with_tools.invoke(chat_history)\n",
    "chat_history.append(response_1)\n",
    "\n",
    "print(type(response_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `invoke(inputs)` method, you get a response from the model. You add the response to the chat history. The code block also prints out the type of the response which is the `AIMessage` class from LangChain. Uncomment the second print statement and read through the fields of the `AIMessage` response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse tool calls\n",
    "\n",
    "Now that you have the response from the model, you can parse the response for tool calling instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool name:\n",
      "add\n",
      "tool args:\n",
      "{'a': 3, 'b': 2}\n",
      "tool call ID:\n",
      "call_BHU0F4CBYAkbAHcKDgKXV01j\n"
     ]
    }
   ],
   "source": [
    "tool_calls_1 = response_1.tool_calls\n",
    "\n",
    "tool_1_name = tool_calls_1[0][\"name\"]\n",
    "tool_1_args = tool_calls_1[0][\"args\"]\n",
    "tool_call_1_id = tool_calls_1[0][\"id\"]\n",
    "\n",
    "print(f'tool name:\\n{tool_1_name}')\n",
    "print(f'tool args:\\n{tool_1_args}')\n",
    "print(f'tool call ID:\\n{tool_call_1_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extracting the `name` from the first call gives the name of the tool to use.\n",
    "    - `add` in this case\n",
    "- Extracting the `args` gives the inputs to pass into the tool.\n",
    "    - `{a: 3, b: 2}` in this case\n",
    "- Extracting the `id` gives the unique identifier for the tool call\n",
    "    - The ID will be different each time, linking tool calls to their respective responses\n",
    "    - Crucial in differentiating calls to the same tool and parallel tool calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the Tool\n",
    "\n",
    "Given the tool call details from the LLM, invoke the correct tool with the correct arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='5' tool_call_id='call_BHU0F4CBYAkbAHcKDgKXV01j'\n"
     ]
    }
   ],
   "source": [
    "tool_response = tool_map[tool_1_name].invoke(tool_1_args)\n",
    "tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_1_id)\n",
    "\n",
    "print(tool_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `tool_map`, passing in the tool name and parameters to get a response. Then wrap that response in a `ToolMessage` object from LangChain along with the tool call ID. This action allows the model and LangChain to better process tool responses and overall conversation between user and model and tool. Feel free to uncomment the print statement to see what the `tool_message` looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append(tool_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, append the `tool_message` to the `chat_history` so the model preserves context and sees prior conversation for a better conversing experience. Now the chat history contains a `HumanMessage` (initial user query), an `AIMessage` (the response from the model), and a `ToolMessage` (the output of the tool).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a final answer from chat history\n",
    "\n",
    "As a final step, pass the entire `chat_history` into the LLM one more time to get a final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "3 + 2 equals 5.\n"
     ]
    }
   ],
   "source": [
    "answer = llm_with_tools.invoke(chat_history)\n",
    "print(type(answer))\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the `answer.content` (content field of the `AIMessage` object) gives the final result of the LLM for the user query. You have finished a complete interaction between the user and model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Agent\n",
    "\n",
    "You can wrap all the prior functionality in a unified Agent class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallingAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm_with_tools = llm.bind_tools(tools)\n",
    "        self.tool_map = tool_map\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        # Step 1: Initial user message\n",
    "        chat_history = [HumanMessage(content=query)]\n",
    "\n",
    "        # Step 2: LLM chooses tool\n",
    "        response = self.llm_with_tools.invoke(chat_history)\n",
    "        if not response.tool_calls:\n",
    "            return response.contet # Direct response, no tool needed\n",
    "        # Step 3: Handle first tool call\n",
    "        tool_call = response.tool_calls[0]\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_call_id = tool_call[\"id\"]\n",
    "\n",
    "        # Step 4: Call tool manually\n",
    "        tool_result = self.tool_map[tool_name].invoke(tool_args)\n",
    "\n",
    "        # Step 5: Send result back to LLM\n",
    "        tool_message = ToolMessage(content=str(tool_result), tool_call_id=tool_call_id)\n",
    "        chat_history.extend([response, tool_message])\n",
    "\n",
    "        # Step 6: Final LLM result\n",
    "        final_response = self.llm_with_tools.invoke(chat_history)\n",
    "        return final_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent does the exact same process as above except the interaction with the model handling is all contained within the `run()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One plus two equals three.\n",
      "The result of \\(1 - 2\\) is \\(-1\\).\n",
      "Three times two equals 6.\n"
     ]
    }
   ],
   "source": [
    "my_agent = ToolCallingAgent(llm)\n",
    "\n",
    "print(my_agent.run(\"one plus 2\"))\n",
    "\n",
    "print(my_agent.run(\"one - 2\"))\n",
    "\n",
    "print(my_agent.run(\"three times two\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three examples of the agent in use. These agents are dynamic and can handle many different types and formats of data as input. This capability is a major benefit of AI agents as the input data doesn't need to be normalized or formatted a certain way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now completed this short introduction to building interactive tool calling agents. Now you can:\n",
    "- Structure user interactions and setup chat models for real-time, context-aware conversations\n",
    "- Extracte tool names and arguments to precisely match user intent\n",
    "- Parse complex tool instructions, including handling multiple tool calls\n",
    "- Build and refine an agent class to automate the entire tool-calling process\n",
    "- Dempnstrate how these components work together to transform LLMs from passive responders to intelligent agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create a new tool\n",
    "\n",
    "Use the example tool format provided in the notebook to create a new tool named `calculate_tip` that takes a `total_bill and tip_percent`, and returns the tip amount. </br>\n",
    "Define and invoke the tool with sample inputs like `total_bill=120`, `tip_percent=15`. </br>\n",
    "Create a `tool_map` with the `calculate_tip` tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def calculate_tip(total_bill: int, tip_percent: int) -> int:\n",
    "    \"\"\"Calculate tip\"\"\"\n",
    "    return total_bill * tip_percent * 0.01\n",
    "\n",
    "inputs = {\n",
    "    \"total_bill\": 120,\n",
    "    \"tip_percent\": 15\n",
    "}\n",
    "calculate_tip.invoke(inputs)\n",
    "\n",
    "\n",
    "tool_map = {\n",
    "    \"calculate_tip\": calculate_tip\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Tool calling with an LLM\n",
    "\n",
    "Simulate a user query like \"How much should I tip on $60 at 20%?\". </br>\n",
    "Bind the tool to the predefined `llm` and prompt the LLM with the query above. Then parse the LLM response for the tool calling details and invoke the tool accordingly. Finally, take the entire chat history and prompt the LLM for a final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2\n",
    "query = \"How much should I tip on $60 at 20%?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "llm_with_tool = llm.bind_tools([calculate_tip])\n",
    "chat_history = [HumanMessage(content=query)]\n",
    "\n",
    "response = llm_with_tool.invoke(chat_history)\n",
    "\n",
    "tool_calls = response.tool_calls\n",
    "tool_name = tool_calls[0][\"name\"]\n",
    "tool_args = tool_calls[0][\"args\"]\n",
    "tool_call_id = tool_calls[0][\"id\"]\n",
    "\n",
    "tool_response = tool_map[tool_name].invoke(tool_args)\n",
    "tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_id)\n",
    "\n",
    "chat_history.extend([response, tool_message])\n",
    "\n",
    "result = llm_with_tool.invoke(chat_history)\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a tip calculating agent\n",
    "\n",
    "Create an agent to automate the entire process you previously completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 3\n",
    "query = \"How much should I tip on $60 at 20%?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "class TipAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm_with_tool = llm.bind_tools([calculate_tip])\n",
    "        self.tool_map = tool_map\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        chat_history = [HumanMessage(content=query)]\n",
    "        response = llm_with_tool.invoke(chat_history)\n",
    "\n",
    "        tool_calls = response.tool_calls\n",
    "        tool_name = tool_calls[0][\"name\"]\n",
    "        tool_args = tool_calls[0][\"args\"]\n",
    "        tool_call_id = tool_calls[0][\"id\"]\n",
    "        \n",
    "        tool_response = tool_map[tool_name].invoke(tool_args)\n",
    "        tool_message = ToolMessage(content=tool_response, tool_call_id=tool_call_id)\n",
    "        \n",
    "        chat_history.extend([response, tool_message])\n",
    "        \n",
    "        return llm_with_tool.invoke(chat_history).content\n",
    "\n",
    "agent = TipAgent(llm)\n",
    "agent.run(\"How much should I tip on $60 at 20%?\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joshua Zhou](https://author.skills.network/instructors/joshua_zhou)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kunal Makwana](https://author.skills.network/instructors/kunal_makwana)</br>\n",
    "[Karan Goswami](https://author.skills.network/instructors/karan_goswami)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Changelog\n",
    "\n",
    "| Date | Version | Changed by | Change Description |\n",
    "|------|--------|--------|---------|\n",
    "| 2024-06-06 | 0.1 |  P. Kravitz | ID review and edit. No code edits.Updated the copyright statement. Change log added. Instructional edits only for IBM style. Second person, accessibility, and other minor grammar edits.| -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "3ed25488039fa4c590a60c6be98ebd97e5fa32f9525446191ec94fcf70e9fc62"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
